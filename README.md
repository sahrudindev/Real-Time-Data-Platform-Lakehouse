# Enterprise Data Platform

![License](https://img.shields.io/badge/license-MIT-blue.svg)
![Docker](https://img.shields.io/badge/docker-ready-blue.svg)
![Python](https://img.shields.io/badge/python-3.10+-green.svg)

A production-ready, enterprise-grade **Real-Time Data Platform** with Lakehouse architecture, Data Governance, and ML-ready capabilities.

## ğŸ—ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         DATA INGESTION LAYER                             â”‚
â”‚  Kafka Streams  â”‚  Debezium CDC  â”‚  FastAPI REST  â”‚  Batch Files        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      REAL-TIME PROCESSING (Spark)                        â”‚
â”‚  Watermarking  â”‚  Deduplication  â”‚  State Management  â”‚  Checkpoints    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LAKEHOUSE LAYER (Delta Lake)                          â”‚
â”‚       BRONZE (Raw)  â”€â”€â”€â–¶  SILVER (Cleansed)  â”€â”€â”€â–¶  GOLD (Aggregated)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SERVING: Metabase (BI)  â”‚  Feast (Features)  â”‚  FastAPI (REST)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

> **Note:** Untuk panduan lengkap instalasi di laptop baru, lihat [docs/USAGE_GUIDE.md](docs/USAGE_GUIDE.md).

```bash
# Clone the repository
git clone https://github.com/your-repo/enterprise-data-platform.git
cd enterprise-data-platform

# Setup Environment Credentials
cd docker
cp .env.example .env

# Start all services (Stable Version)
docker compose -f docker-compose-no-spark.yml up -d

# Verify services are running
docker compose -f docker-compose-no-spark.yml ps
```

## ğŸ“¦ Tech Stack

| Layer | Technology |
|-------|------------|
| Streaming | Apache Kafka, Debezium |
| Processing | Apache Spark 3.5 |
| Storage | Delta Lake, MinIO (S3) |
| Transformation | dbt |
| Orchestration | Apache Airflow |
| Quality | Great Expectations |
| Governance | DataHub |
| ML Features | Feast |
| BI | Metabase |
| Monitoring | Prometheus, Grafana |
| API | FastAPI |

## ğŸ“ Project Structure

```
â”œâ”€â”€ docker/                 # Docker Compose & configs
â”œâ”€â”€ kafka/                  # Kafka producers, topics
â”œâ”€â”€ spark/                  # Spark streaming jobs
â”‚   â”œâ”€â”€ jobs/              # Processing scripts
â”‚   â””â”€â”€ utils/             # Delta Lake utilities
â”œâ”€â”€ dbt/                    # dbt models & tests
â”œâ”€â”€ airflow/                # Airflow DAGs
â”œâ”€â”€ great_expectations/     # Data quality suites
â”œâ”€â”€ feature_store/          # Feast definitions
â”œâ”€â”€ api/                    # FastAPI REST service
â”œâ”€â”€ monitoring/             # Prometheus & Grafana
â”œâ”€â”€ website/                # Next.js portfolio
â””â”€â”€ docs/                   # Documentation
```

## ğŸŒ Service URLs

| Service | URL | Credentials |
|---------|-----|-------------|
| Airflow | http://localhost:8080 | admin / admin |
| Spark UI | http://localhost:8082 | - |
| Kafka UI | http://localhost:8081 | - |
| Metabase | http://localhost:3000 | signup |
| Grafana | http://localhost:3001 | admin / admin |
| MinIO | http://localhost:9001 | minioadmin / minioadmin123 |
| API Docs | http://localhost:8000/docs | - |
| Jupyter | http://localhost:8888 | token: enterprise_data_platform |

## âš¡ Running Components

### Start Streaming Ingestion
```bash
docker exec -it spark-master spark-submit \
  --master spark://spark-master:7077 \
  --packages io.delta:delta-core_2.12:2.4.0 \
  /opt/spark/jobs/streaming_ingestion.py
```

### Run dbt Models
```bash
docker exec -it airflow-worker bash -c "cd /opt/dbt && dbt run && dbt test"
```

### Validate Data Quality
```bash
docker exec -it airflow-worker bash -c \
  "great_expectations checkpoint run bronze_orders_checkpoint"
```

### Produce Test Events
```bash
docker exec -it api python -c "
from kafka.producers.event_producer import main
main()
"
```

## ğŸ“Š Data Flow

1. **Ingestion**: Events â†’ Kafka topics
2. **Processing**: Spark Streaming with exactly-once semantics
3. **Bronze**: Raw event landing (full history)
4. **Silver**: Cleansed, deduplicated, SCD2
5. **Gold**: Business aggregations, metrics
6. **Serving**: BI dashboards, ML features, API

## ğŸ”§ Configuration

Key environment variables in `docker/.env`:

```bash
KAFKA_BOOTSTRAP_SERVERS=kafka:9092
SPARK_MASTER=spark://spark-master:7077
DELTA_LAKE_PATH=/data/delta
```

## ğŸ“ˆ Monitoring

- **Prometheus**: http://localhost:9090
- **Grafana**: http://localhost:3001 (dashboards pre-configured)
- Alerts configured for: Pipeline down, High error rate, Kafka lag

## ğŸ¨ Portfolio Website

```bash
cd website
npm install
npm run dev
# Open http://localhost:3000
```

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) file.
